![PyTorch](https://img.shields.io/badge/PyTorch-%23EE4C2C.svg?style=for-the-badge&logo=PyTorch&logoColor=white)


# **MT4MTL-KD**: A Multi-teacher Knowledge Distillation Framework for Triplet Recognition

<i>Shuangchun Gui, Zhenkun Wang, Jixiang Chen, Xun Zhou, Chen Zhang, and Yi Cao</i>

<img src="imgs/3_metd_framework.jpg" width="100%">

This repository contains the implementation code, inference demo, and evaluation scripts. <br />


# Performance

## Results Table


||Components AP ||||| Association AP |||
:---:|:---:|:---:|:---: |:---:|:---:|:---:|:---:|:---:|
AP<sub>I</sub> | AP<sub>V</sub> | AP<sub>T</sub> ||| AP<sub>IV</sub> | AP<sub>IT</sub> | AP<sub>IVT</sub> |
89.87 | 70.60 | 50.20 ||| 41.84 | 44.25 | 35.88 |

<br />



# Installation

## Requirements

The model depends on the following libraries:

<br />

## System Requirements:
The code has been test on Linux operating system. It runs on both CPU and GPU.
Equivalence of basic OS commands such as _unzip, cd, wget_, etc. will be needed to run in Windows or Mac OS.

<br />
